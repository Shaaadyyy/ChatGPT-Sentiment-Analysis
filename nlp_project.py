# -*- coding: utf-8 -*-
"""NLP_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s-0MExbkoFWx5V5o_Tjb6oRdcKdiHr4N
"""

from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.stem import WordNetLemmatizer
import pandas as pd
import nltk
import numpy as np
import matplotlib.pyplot as plt
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from google.colab import drive
from keras.layers import Embedding
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten
from keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import *
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from keras.utils import to_categorical
from keras.optimizers import *
drive.mount('/content/drive')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

#reading data file
data = pd.read_csv('/content/drive/MyDrive/file.csv')
label_map = {'bad': 0, 'neutral': 1, 'good': 2}

data['labels'] = data['labels'].map(label_map)

print(data)

x = data['tweets']
y = data['labels']
print(x)
print('/////////////////////')
print(y)

for key, ID in label_map.items():
  print(key, ID)

plt.hist(data['labels'], edgecolor='black',bins=20)
plt.title("Points histogram")
plt.ylabel("labels")
plt.xlabel("tweets")
plt.show()

#tokenization & padding
MAX_SEQUENCE_LENGTH = 1000
MAX_NUM_WORDS = 20000
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(x)
sequences = tokenizer.texts_to_sequences(x)
word_index = tokenizer.word_index # the dictionary
print('Found %s unique tokens.' % len(word_index))
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

labels_matrix = to_categorical(np.asarray(y))

x_train, x_val, y_train, y_val = train_test_split(data, labels_matrix, test_size=0.2, stratify=labels_matrix)
print('Shape of training data:', x_train.shape)
print('Shape of validation data:', x_val.shape)

VALIDATION_SPLIT = 0.2
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data_shuffled = data[indices]
labels_shuffled = labels_matrix[indices]
nb_validation_samples = int(VALIDATION_SPLIT * data_shuffled.shape[0])
x_train = data_shuffled[:-nb_validation_samples]
y_train = labels_shuffled[:-nb_validation_samples]
x_val = data_shuffled[-nb_validation_samples:]
y_val = labels_shuffled[-nb_validation_samples:]
print('Shape of training data: ',x_train.shape)
print('Shape of testing data: ',x_val.shape)

EMBEDDING_DIM = 100
print('Indexing word vectors.')
embeddings_index = {}
with open('/content/drive/MyDrive/glove.6B.100d.txt') as f:
  for line in f:
    values = line.split(sep=' ')
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
print('Found %s word vectors.' % len(embeddings_index))

embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
#+1 to include the zeros vector for non-existing words
for word, i in word_index.items():
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    # words not found in embedding index will be all-zeros.
    embedding_matrix[i] = embedding_vector
print ('Shape of Embedding Matrix: ',embedding_matrix.shape)

model_1 = Sequential()
model_1.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,))
model_1.add(layers.Dropout(0.5))
model_1.add(Conv1D(16,3, activation='relu'))
model_1.add(MaxPooling1D())
model_1.add(Conv1D(16,3, activation='relu'))
model_1.add(MaxPooling1D())
model_1.add(layers.Flatten())

var = 0
acc = []
for i in range(0,4):
  if(i == 0):
    var = 128
  elif(i == 1):
    var = 256
  elif(i == 2):
    var = 512
  else:
    var = 1024
  model_1.add(layers.Dense(var, activation='relu')) #number of neurons in dense layer
  model_1.add(layers.Dropout(0.5))
  model_1.add(layers.Dense(3, activation='sigmoid'))
  model_1.compile(loss='categorical_crossentropy',
  optimizer='rmsprop',
  metrics=['acc'])

  history = model_1.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)
  print('Acuracy on testing set:', i)
  accuracyVar = model_1.evaluate(x_val,y_val)
  acc.append(accuracyVar)
print(acc)

# Plot accuracy vs trials
accList = []
for i in acc:
  accList.append(i[1])
x = range(len(accList))
plt.plot(x, accList)
custom_labels = ['trial 1', 'trial 2', 'trial 3', 'trial 4']
plt.xticks(x, custom_labels)
plt.title('Model accuracy over trials')
plt.ylabel('Accuracy')
plt.xlabel('Trials')
plt.legend(['Train'], loc='upper left')
plt.grid(True)
plt.show()

def prediction(model,tweet):
  sequences_ = tokenizer.texts_to_sequences(tweet)
  data_p = pad_sequences(sequences_, maxlen=MAX_SEQUENCE_LENGTH)
  label_vec = model.predict(data_p)
  label_num = np.argmax(label_vec)
  label_type = ""
  if(label_num == 0):
    label_type = "bad"
  elif(label_num == 1):
    label_type = "neutral"
  else:
    label_type = "good"
  return tweet , label_type

inputuser = input("enter a sentence: ")
tweet, labeltype = prediction(model_1,[inputuser])
print(tweet)
print(labeltype)

model_2 = Sequential()
model_2.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,))
model_2.add(layers.Dropout(0.5))
model_2.add(LSTM(16, return_sequences=True))
model_2.add(layers.Dropout(0.5))
model_2.add(LSTM(16))
model_2.add(layers.Dropout(0.5))

var2 = 0
accuracies = []
for i in range(0,4):
  if(i == 0):
    var2 = 128
  elif(i == 1):
    var2 = 256
  elif(i == 2):
    var = 512
  else:
    var2 = 1024
  model_2.add(layers.Dense(var2, activation='relu'))
  model_2.add(layers.Dropout(0.5))
  model_2.add(layers.Dense(3, activation='sigmoid'))
  model_2.compile(loss='categorical_crossentropy',
  optimizer='rmsprop',
  metrics=['acc'])
  history = model_2.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)
  print('Acuracy on testing set:', i)
  accuracyVar = model_2.evaluate(x_val,y_val)
  accuracies.append(accuracyVar)
print(accuracies)

# Plot accuracy vs trials
accList2 = []
for i in accuracies:
  accList2.append(i[1])
x = range(len(accList2))
plt.plot(x, accList2)
custom_labels = ['trial 1', 'trial 2', 'trial 3', 'trial 4']
plt.xticks(x, custom_labels)
plt.title('Model2 accuracy over trials')
plt.ylabel('Accuracy')
plt.xlabel('Trials')
plt.legend(['Train'], loc='upper left')
plt.grid(True)
plt.show()

inputuser = input("enter a sentence: ")
tweet, labeltype = prediction(model_2,[inputuser])
print(tweet)
print(labeltype)